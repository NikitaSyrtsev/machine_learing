{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTreeClf:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2, max_leafs=20, bins=None, criterion='entropy'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_leafs = max_leafs\n",
    "        self.leafs_cnt = 0  # Переменная для отслеживания количества листьев\n",
    "        self.tree = None  # Дерево будем хранить здесь\n",
    "        self.bins = bins\n",
    "        self.bin_edges = {}  # Словарь для хранения границ бинов для каждой фичи\n",
    "        self.criterion = criterion  # Новый параметр для выбора критерия\n",
    "        self.fi = {}  # Словарь для хранения важности фичей\n",
    "    \n",
    "    def __str__(self):\n",
    "        params = vars(self) # Получаем все атрибуты экземпляра как словарь\n",
    "        params_str = ', '.join(f\"{key}={value}\" for key, value in params.items())\n",
    "        return f\"MyTreeClf class: {params_str}\"\n",
    "    \n",
    "    def _initialize_bins(self, X):\n",
    "        for col in X.columns:\n",
    "            if self.bins is not None:\n",
    "                unique_values = np.unique(X[col])\n",
    "                if len(unique_values) <= self.bins - 1:\n",
    "                    self.bin_edges[col] = unique_values\n",
    "                else:\n",
    "                    hist, edges = np.histogram(X[col], bins=self.bins)\n",
    "                    # Используем только внутренние границы\n",
    "                    self.bin_edges[col] = edges[1:-1]\n",
    "                    \n",
    "       # Вычисление неопределенности Джини\n",
    "    def _gini(self, y):\n",
    "        value_counts = np.bincount(y)  # Подсчет количества каждого класса\n",
    "        probabilities = value_counts / len(y)  # Вероятности классов\n",
    "        gini = 1 - np.sum(probabilities ** 2)  # Формула Джини\n",
    "        return gini\n",
    "    \n",
    "    # Вычисление энтропии\n",
    "    def _entropy(self, y):\n",
    "        value_counts = np.bincount(y)  # Подсчёт количества каждого класса\n",
    "        probabilities = value_counts / len(y)  # Вероятности классов\n",
    "        # Для обработки логарифмов от 0 используем условие (если вероятность 0, то член равен 0)\n",
    "        entropy = -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "        return entropy\n",
    "\n",
    "    def get_best_split(self, X: pd.DataFrame, y: pd.Series):\n",
    "        best_split = {\n",
    "            \"col_name\": None,\n",
    "            \"split_value\": None,\n",
    "            \"ig\": -1  # Information Gain (Прирост информации)\n",
    "        }\n",
    "        \n",
    "        # Вычисление базовой неопределенности в зависимости от критерия\n",
    "        if self.criterion == 'entropy':\n",
    "            base_uncertainty = self._entropy(y)\n",
    "        elif self.criterion == 'gini':\n",
    "            base_uncertainty = self._gini(y)\n",
    "\n",
    "        # Перебор всех колонок (фичей)\n",
    "        for col in X.columns:\n",
    "            # Получаем разделители для данной фичи\n",
    "            if col in self.bin_edges:\n",
    "                unique_values = self.bin_edges[col]  # Используем границы бинов\n",
    "            else:\n",
    "                unique_values = np.sort(X[col].unique())  # Используем уникальные значения\n",
    "\n",
    "            # Перебор возможных значений для разделения\n",
    "            for i in range(1, len(unique_values)):\n",
    "                split_value = (unique_values[i - 1] + unique_values[i]) / 2  # Разделитель как среднее между двумя значениями\n",
    "\n",
    "                # Левая и правая подвыборки\n",
    "                left_mask = X[col] <= split_value\n",
    "                right_mask = X[col] > split_value\n",
    "\n",
    "                left_y, right_y = y[left_mask], y[right_mask]\n",
    "\n",
    "                # Пропускаем разбиение, если одна из подвыборок пуста\n",
    "                if len(left_y) == 0 or len(right_y) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Вычисляем неопределенность для левой и правой подвыборки\n",
    "                if self.criterion == 'entropy':\n",
    "                    left_uncertainty = self._entropy(left_y)\n",
    "                    right_uncertainty = self._entropy(right_y)\n",
    "                elif self.criterion == 'gini':\n",
    "                    left_uncertainty = self._gini(left_y)\n",
    "                    right_uncertainty = self._gini(right_y)\n",
    "\n",
    "                # Взвешенная неопределенность\n",
    "                weighted_uncertainty = (len(left_y) / len(y)) * left_uncertainty + (len(right_y) / len(y)) * right_uncertainty\n",
    "\n",
    "                # Прирост информации (Information Gain)\n",
    "                information_gain = base_uncertainty - weighted_uncertainty\n",
    "\n",
    "                # Сравнение с текущим лучшим разделителем\n",
    "                if information_gain > best_split[\"ig\"]:\n",
    "                    best_split[\"col_name\"] = col\n",
    "                    best_split[\"split_value\"] = split_value\n",
    "                    best_split[\"ig\"] = information_gain\n",
    "\n",
    "            return best_split[\"col_name\"], best_split[\"split_value\"], best_split[\"ig\"]\n",
    "    \n",
    "     # Метод fit для построения дерева\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        self.leafs_cnt = 0  # Сброс счетчика листьев\n",
    "        self.fi = {col: 0 for col in X.columns}  # Инициализация важности фичей\n",
    "        \n",
    "         # Инициализация разделителей\n",
    "        self._initialize_bins(X)\n",
    "\n",
    "        # Вложенная рекурсивная функция для построения дерева\n",
    "        def build_tree(X, y, depth=0):\n",
    "            # Условия остановки\n",
    "            if len(np.unique(y)) == 1:  # Все элементы одного класса\n",
    "                self.leafs_cnt += 1\n",
    "                return np.unique(y)[0]\n",
    "            \n",
    "            if depth >= self.max_depth or len(y) < self.min_samples_split or self.leafs_cnt >= self.max_leafs:\n",
    "                self.leafs_cnt += 1\n",
    "                return np.bincount(y).argmax()  # Возвращаем наиболее частый класс\n",
    "\n",
    "            # Поиск лучшего разбиения\n",
    "            col_name, split_value, ig = self.get_best_split(X, y)\n",
    "\n",
    "            if ig == -1 or self.leafs_cnt >= self.max_leafs:  # Если прироста информации нет или лимит листьев достигнут, создаём новый лист\n",
    "                self.leafs_cnt += 1\n",
    "                return np.bincount(y).argmax()\n",
    "            \n",
    "            self.fi[col_name] += ig  # Увеличиваем важность на величину прироста информации\n",
    "\n",
    "            # Разделение данных\n",
    "            left_mask = X[col_name] <= split_value\n",
    "            right_mask = X[col_name] > split_value\n",
    "\n",
    "            left_tree = build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "            right_tree = build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "            # Структура узла\n",
    "            return {\n",
    "                \"col_name\": col_name,\n",
    "                \"split_value\": split_value,\n",
    "                \"left\": left_tree,\n",
    "                \"right\": right_tree\n",
    "            }\n",
    "\n",
    "        # Строим дерево\n",
    "        self.tree = build_tree(X, y)\n",
    "        \n",
    "    def feature_importances(self):\n",
    "        return self.fi\n",
    "        \n",
    "     # Метод для печати дерева\n",
    "    def print_tree(self, node=None, depth=0):\n",
    "        if node is None:\n",
    "            node = self.tree\n",
    "        \n",
    "        # Если это лист (значение), выводим его\n",
    "        if not isinstance(node, dict):\n",
    "            print(f\"{'  ' * depth}leaf = {node}\")\n",
    "            return\n",
    "        \n",
    "        # Если это узел, выводим информацию о разбиении\n",
    "        print(f\"{'  ' * depth}{node['col_name']} > {node['split_value']}\")\n",
    "        \n",
    "        # Рекурсивно печатаем левую и правую ветви\n",
    "        print(f\"{'  ' * (depth + 1)}Left:\")\n",
    "        self.print_tree(node['left'], depth + 2)\n",
    "        \n",
    "        print(f\"{'  ' * (depth + 1)}Right:\")\n",
    "        self.print_tree(node['right'], depth + 2)\n",
    "        \n",
    "    # Метод для предсказания вероятностей\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "        probabilities = []\n",
    "        \n",
    "        for _, row in X.iterrows():\n",
    "            probabilities.append(self._predict_proba_single(row, self.tree))\n",
    "        \n",
    "        return np.array(probabilities)\n",
    "\n",
    "    # Вспомогательный метод для предсказания одной строки\n",
    "    def _predict_proba_single(self, row, node):\n",
    "        # Если это лист (значение), возвращаем вероятность\n",
    "        if not isinstance(node, dict):\n",
    "            return node\n",
    "\n",
    "        # Переход к следующему узлу\n",
    "        col_name = node['col_name']\n",
    "        split_value = node['split_value']\n",
    "\n",
    "        if row[col_name] <= split_value:\n",
    "            return self._predict_proba_single(row, node['left'])\n",
    "        else:\n",
    "            return self._predict_proba_single(row, node['right'])\n",
    "\n",
    "    # Метод для предсказания классов\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature1': 0.7338578863016243, 'feature2': 0, 'feature3': 0}\n"
     ]
    }
   ],
   "source": [
    "# Важность фичей\n",
    "X = pd.DataFrame({\n",
    "    'feature1': [1, 2, 3, 4, 5],\n",
    "    'feature2': [5, 4, 3, 2, 1],\n",
    "    'feature3': [1, 3, 5, 7, 9]\n",
    "})\n",
    "\n",
    "y = pd.Series([0, 1, 0, 1, 0])\n",
    "\n",
    "# Инициализация и обучение модели\n",
    "model = MyTreeClf(max_depth=3, criterion='entropy')\n",
    "model.fit(X, y)\n",
    "\n",
    "# Получение важности фичей\n",
    "importances = model.feature_importances()\n",
    "print(importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сумма: 104 и 100\n"
     ]
    }
   ],
   "source": [
    "#Тестирование предсказания\n",
    "X, y = make_classification(n_samples=100, n_features=5, n_informative=3, n_redundant=0, random_state=42)\n",
    "X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "y = pd.Series(y)\n",
    "\n",
    "my_clf1 = MyTreeClf(max_depth=3, min_samples_split=2, max_leafs=10, criterion='entropy')\n",
    "my_clf1.fit(X, y)\n",
    "my_clf2 = MyTreeClf(max_depth=4, min_samples_split=3, max_leafs=30, criterion='gini')\n",
    "my_clf2.fit(X, y)\n",
    "\n",
    "# Получение вероятностей и предсказаний\n",
    "my_probabilities1 = my_clf1.predict_proba(X)\n",
    "my_predictions1 = my_clf1.predict(X)\n",
    "my_probabilities2 = my_clf2.predict_proba(X)\n",
    "my_predictions2 = my_clf2.predict(X)\n",
    "\n",
    "# Сумма вероятностей\n",
    "sum_my_clf1 = np.sum(my_predictions1) + np.sum(my_probabilities1)\n",
    "sum_my_clf2 = np.sum(my_predictions2) + np.sum(my_probabilities2)\n",
    "\n",
    "print(f\"Сумма: {sum_my_clf1} и {sum_my_clf2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Построенное дерево:\n",
      "feature_3 > 0.10692195695825762\n",
      "  Left:\n",
      "    feature_2 > 0.5037529278758279\n",
      "      Left:\n",
      "        feature_3 > -0.4069952905845129\n",
      "          Left:\n",
      "            leaf = 0\n",
      "          Right:\n",
      "            leaf = 0\n",
      "      Right:\n",
      "        leaf = 1\n",
      "  Right:\n",
      "    feature_1 > -0.6408632697678901\n",
      "      Left:\n",
      "        feature_3 > 1.4043361004297479\n",
      "          Left:\n",
      "            leaf = 0\n",
      "          Right:\n",
      "            leaf = 1\n",
      "      Right:\n",
      "        feature_3 > 0.5167263246823051\n",
      "          Left:\n",
      "            leaf = 1\n",
      "          Right:\n",
      "            leaf = 1\n",
      "\n",
      "Количество листьев:\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#Тестирование обучения\n",
    "X, y = make_classification(n_samples=100, n_features=5, n_informative=3, n_redundant=2, random_state=42)\n",
    "\n",
    "# Преобразуем в DataFrame и Series\n",
    "X_test = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "y_test = pd.Series(y)\n",
    "\n",
    "# Создаем объект класса\n",
    "clf = MyTreeClf(max_depth=3, min_samples_split=2, max_leafs=6)\n",
    "\n",
    "# Вызываем метод fit для теста\n",
    "clf.fit(X_test, y_test)\n",
    "\n",
    "# Печатаем дерево\n",
    "print(\"Построенное дерево:\")\n",
    "clf.print_tree()\n",
    "\n",
    "# Проверяем количество листьев\n",
    "print(\"\\nКоличество листьев:\")\n",
    "print(clf.leafs_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший сплит:\n",
      "Фича: col_3\n",
      "Значение для разделения: 0.20389423213510222\n",
      "Прирост информации: 0.5720798775568654\n"
     ]
    }
   ],
   "source": [
    "#Тестирование поиска наилучшего сплита\n",
    "# Создаем тестовый набор данных\n",
    "X, y = make_classification(n_samples=100, n_features=5, n_informative=3, random_state=42)\n",
    "\n",
    "# Преобразуем X в DataFrame для удобства работы с нашим методом\n",
    "X_df = pd.DataFrame(X, columns=[f'col_{i}' for i in range(X.shape[1])])\n",
    "\n",
    "# Разделение на тренировочную и тестовую выборки для проверки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Инициализируем наш класс\n",
    "clf = MyTreeClf()\n",
    "\n",
    "# Применяем метод get_best_split на тренировочных данных\n",
    "best_split = clf.get_best_split(X_train, y_train)\n",
    "\n",
    "# Выводим результат\n",
    "print(\"Лучший сплит:\")\n",
    "print(f\"Фича: {best_split[0]}\")\n",
    "print(f\"Значение для разделения: {best_split[1]}\")\n",
    "print(f\"Прирост информации: {best_split[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyTreeClf class: max_depth=5, min_samples_split=2, max_leafs=20\n",
      "MyTreeClf class: max_depth=10, min_samples_split=2, max_leafs=30\n",
      "MyTreeClf class: max_depth=5, min_samples_split=15, max_leafs=25\n"
     ]
    }
   ],
   "source": [
    "# Тестирование класса\n",
    "tree1 = MyTreeClf()\n",
    "tree2 = MyTreeClf(max_depth = 10, max_leafs = 30)\n",
    "tree3 = MyTreeClf(max_leafs = 25, min_samples_split = 15)\n",
    "\n",
    "# Проверка\n",
    "print(tree1)\n",
    "print(tree2)\n",
    "print(tree3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
