{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogReg:\n",
    "    def __init__(self, n_iter=100, learning_rate=0.1, metric=None, reg=None, l1_coef=0, l2_coef=0, sgd_sample=None, random_state=42):\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = None\n",
    "        self.metric = metric\n",
    "        self.best_score = None \n",
    "        self.reg = reg\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "        self.sgd_sample = sgd_sample\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def __str__(self):\n",
    "        params = vars(self) # Получаем все атрибуты экземпляра как словарь\n",
    "        params_str = ', '.join(f\"{key}={value}\" for key, value in params.items())\n",
    "        return f\"MyLogReg class: {params_str}\"\n",
    "    \n",
    "    # Функция ошибки (Log Loss)\n",
    "    def log_loss(self, y_true, y_pred):\n",
    "        epsilon = 1e-15  # Маленькое число для предотвращения деления на 0\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Ограничиваем предсказание\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    # сигмоидная функция активации для вероятностей от 0 до 1\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    #Добавление регуляризации к функции потерь.\n",
    "    def regularization_loss(self, loss):\n",
    " \n",
    "        if self.reg == 'l1':\n",
    "            regularization = self.l1_coef * np.sum(np.abs(self.weights[1:]))\n",
    "        elif self.reg == 'l2':\n",
    "            regularization = self.l2_coef * np.sum(self.weights[1:] ** 2)\n",
    "        elif self.reg == 'elasticnet':\n",
    "            l1_reg = self.l1_coef * np.sum(np.abs(self.weights[1:]))\n",
    "            l2_reg = self.l2_coef * np.sum(self.weights[1:] ** 2)\n",
    "            regularization = l1_reg + l2_reg\n",
    "        else:\n",
    "            regularization = 0\n",
    "        return loss + regularization\n",
    "\n",
    "\n",
    "    #Добавление регуляризации к градиенту.\n",
    "    def regularization_gradient(self, gradients):\n",
    "        if self.reg == 'l1':\n",
    "            gradients[1:] += self.l1_coef * np.sign(self.weights[1:])  # L1 градиент\n",
    "        elif self.reg == 'l2':\n",
    "            gradients[1:] += 2 * self.l2_coef * self.weights[1:]  # L2 градиент\n",
    "        elif self.reg == 'elasticnet':\n",
    "            gradients[1:] += self.l1_coef * np.sign(self.weights[1:])  # L1 часть\n",
    "            gradients[1:] += 2 * self.l2_coef * self.weights[1:]  # L2 часть\n",
    "        return gradients\n",
    "    \n",
    "     # Метод обучения модели\n",
    "    def fit(self, X, y, verbose=False):\n",
    "        random.seed(self.random_state)\n",
    "         \n",
    "        # Добавляем единичный столбец для смещения\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        \n",
    "        # Инициализация весов: количество фич + 1 (для смещения)\n",
    "        self.weights = np.ones(X.shape[1])\n",
    "        \n",
    "        # Для начального лога ошибки\n",
    "        y_pred_initial = self.sigmoid(np.dot(X, self.weights))\n",
    "        loss_initial = self.log_loss(y, y_pred_initial)\n",
    "        loss_initial = self.regularization_loss(loss_initial)\n",
    "        \n",
    "        # Вывод начальной метрики, если она задана\n",
    "        metric_value_initial = None\n",
    "        if self.metric:\n",
    "            metric_value_initial = self._calculate_metric(y, (y_pred_initial >= 0.5).astype(int), y_pred_initial)\n",
    "            print(f\"Start loss: {loss_initial:.2f} i {self.metric}: {metric_value_initial:.2f}\")\n",
    "        else:\n",
    "            print(f\"Start loss: {loss_initial:.2f}\")\n",
    "        \n",
    "        # Градиентный спуск\n",
    "        for i in range(self.n_iter):\n",
    "            \n",
    "            # Если sgd_sample задан, выбираем подвыборку\n",
    "            if self.sgd_sample:\n",
    "                if isinstance(self.sgd_sample, int):\n",
    "                    sample_size = self.sgd_sample\n",
    "                elif isinstance(self.sgd_sample, float):\n",
    "                    sample_size = int(self.sgd_sample * X.shape[0])\n",
    "                sample_rows_idx = random.sample(range(X.shape[0]), sample_size)\n",
    "                X_sample = X[sample_rows_idx]\n",
    "                y_sample = y[sample_rows_idx]\n",
    "            else:\n",
    "                X_sample = X\n",
    "                y_sample = y\n",
    "                \n",
    "            # 1. Предсказание (sigmoid)\n",
    "            y_pred = self.sigmoid(np.dot(X_sample, self.weights))\n",
    "            \n",
    "            # 2. Вычисление градиента\n",
    "            gradient = np.dot(X_sample.T, (y_pred - y_sample)) / y_sample.size\n",
    "            \n",
    "            # 3. Добавляем регуляризацию к градиенту\n",
    "            gradient = self.regularization_gradient(gradient)\n",
    "            \n",
    "            # 4. Обновление весов\n",
    "            if callable(self.learning_rate):  # Если learning_rate — это функция\n",
    "                current_learning_rate = self.learning_rate(i)\n",
    "            else:  # Если learning_rate — это число\n",
    "                current_learning_rate = self.learning_rate\n",
    "            self.weights -= current_learning_rate * gradient\n",
    "            \n",
    "            # 5. Вычисляем функцию потерь на каждой итерации (вне verbose)\n",
    "            loss = self.log_loss(y, y_pred)\n",
    "            loss = self.regularization_loss(loss)  # Добавляем регуляризацию к лоссу\n",
    "            \n",
    "            # 6. Логирование каждые verbose итераций\n",
    "            if verbose and (i + 1) % verbose == 0:\n",
    "                if self.metric:\n",
    "                    metric_value = self._calculate_metric(y, (y_pred >= 0.5).astype(int), y_pred)\n",
    "                    print(f\"{i + 1} loss: {loss:.2f}, {self.metric}: {metric_value:.2f}\")\n",
    "                else:\n",
    "                    print(f\"{i + 1} loss: {loss:.2f}\")\n",
    "                \n",
    "             # После обучения сохраняем значение метрики для обученной модели\n",
    "            if self.metric:\n",
    "                self.best_score = self._calculate_metric(y, (y_pred >= 0.5).astype(int), y_pred)\n",
    "    \n",
    "    # Метод для получения коэффициентов (весов)\n",
    "    def get_coef(self):\n",
    "        if self.weights is not None:\n",
    "            return self.weights\n",
    "        else:\n",
    "            raise ValueError(\"Model is not fitted yet. Please call the fit method first.\")\n",
    "        \n",
    "     # Метод для предсказания вероятностей классов (логиты через сигмоиду)\n",
    "    def predict_proba(self, X):\n",
    "        # Добавляем единичный столбец для смещения\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        # Возвращаем вероятности через сигмоид\n",
    "        return self.sigmoid(np.dot(X, self.weights))\n",
    "    \n",
    "    # Метод для предсказания классов\n",
    "    def predict(self, X):\n",
    "        # Получаем вероятности\n",
    "        probabilities = self.predict_proba(X)\n",
    "        # Превращаем их в бинарные классы на основе порога 0.5\n",
    "        return (probabilities >= 0.5).astype(int)\n",
    "    \n",
    "     # Реализация Accuracy\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        return np.mean(y_true == y_pred)\n",
    "\n",
    "    # Реализация Precision\n",
    "    def precision(self, y_true, y_pred):\n",
    "        true_positive = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        false_positive = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        if true_positive + false_positive == 0:\n",
    "            return 0.0\n",
    "        return true_positive / (true_positive + false_positive)\n",
    "\n",
    "    # Реализация Recall\n",
    "    def recall(self, y_true, y_pred):\n",
    "        true_positive = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        false_negative = np.sum((y_true == 1) & (y_pred == 0))\n",
    "        if true_positive + false_negative == 0:\n",
    "            return 0.0\n",
    "        return true_positive / (true_positive + false_negative)\n",
    "\n",
    "    # Реализация F1-score\n",
    "    def f1(self, y_true, y_pred):\n",
    "        prec = self.precision(y_true, y_pred)\n",
    "        rec = self.recall(y_true, y_pred)\n",
    "        if prec + rec == 0:\n",
    "            return 0.0\n",
    "        return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "    # Реализация ROC AUC\n",
    "    def roc_auc(self, y_true, y_proba):\n",
    "        # Сортируем по вероятностям\n",
    "        sorted_indices = np.argsort(y_proba)\n",
    "        y_true_sorted = y_true[sorted_indices]\n",
    "        y_proba_sorted = y_proba[sorted_indices]\n",
    "        \n",
    "        # Вычисление метрики ROC AUC вручную\n",
    "        tpr = []  # True positive rate\n",
    "        fpr = []  # False positive rate\n",
    "        \n",
    "        P = np.sum(y_true_sorted == 1)  # количество положительных примеров\n",
    "        N = np.sum(y_true_sorted == 0)  # количество отрицательных примеров\n",
    "        \n",
    "        tp = 0  # True positives\n",
    "        fp = 0  # False positives\n",
    "        \n",
    "        for i in range(len(y_true_sorted)):\n",
    "            if y_true_sorted[i] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "            tpr.append(tp / P)\n",
    "            fpr.append(fp / N)\n",
    "        \n",
    "        # Вычисление площади под кривой (метод трапеций)\n",
    "        auc = np.trapz(tpr, fpr)\n",
    "        return auc\n",
    "    \n",
    "    # Метод для расчёта метрики\n",
    "    def _calculate_metric(self, y_true, y_pred, y_proba):\n",
    "        if self.metric == 'accuracy':\n",
    "            return self.accuracy(y_true, y_pred)\n",
    "        elif self.metric == 'precision':\n",
    "            return self.precision(y_true, y_pred)\n",
    "        elif self.metric == 'recall':\n",
    "            return self.recall(y_true, y_pred)\n",
    "        elif self.metric == 'f1':\n",
    "            return self.f1(y_true, y_pred)\n",
    "        elif self.metric == 'roc_auc':\n",
    "            return self.roc_auc(y_true, y_proba)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric: {self.metric}\")\n",
    "    \n",
    "    # Метод для возврата значения метрики после обучения\n",
    "    def get_best_score(self):\n",
    "        if self.best_score is not None:\n",
    "            return self.best_score\n",
    "        else:\n",
    "            raise ValueError(\"No metric was set or model wasn't trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with sgd_sample = 0.01\n",
      "Start loss: 1.07\n",
      "Среднее значение коэффициентов: 0.4170\n"
     ]
    }
   ],
   "source": [
    "# Генерация синтетических данных\n",
    "X, y = make_classification(n_samples=100, n_features=5, n_classes=2, random_state=42)\n",
    "\n",
    "# Определение параметров для тестирования в диапазоне от 0.0 до 1.0\n",
    "sgd_samples = [0.01]  # Дробные значения представляют долю выборки\n",
    "\n",
    "# Цикл для тестирования с различными параметрами sgd_sample\n",
    "for sgd_sample in sgd_samples:\n",
    "    print(f\"\\nTesting with sgd_sample = {sgd_sample}\")\n",
    "    \n",
    "    # Инициализация модели с текущим значением sgd_sample\n",
    "    model = MyLogReg(n_iter=1000, learning_rate=0.1, sgd_sample=sgd_sample, random_state=42)\n",
    "    \n",
    "    # Обучение модели\n",
    "    model.fit(X, y, verbose=False)\n",
    "    \n",
    "    # Получение средних значений коэффициентов\n",
    "    coef_mean = np.mean(model.get_coef())\n",
    "    print(f\"Среднее значение коэффициентов: {coef_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loss: 2.47 i accuracy: 0.54\n",
      "100 loss: 0.58, accuracy: 0.73\n",
      "200 loss: 0.58, accuracy: 0.73\n",
      "300 loss: 0.58, accuracy: 0.73\n",
      "400 loss: 0.58, accuracy: 0.73\n",
      "500 loss: 0.58, accuracy: 0.73\n",
      "600 loss: 0.58, accuracy: 0.73\n",
      "700 loss: 0.58, accuracy: 0.73\n",
      "800 loss: 0.58, accuracy: 0.73\n",
      "900 loss: 0.58, accuracy: 0.73\n",
      "1000 loss: 0.58, accuracy: 0.73\n",
      "Accuracy with dynamic learning rate: 0.7325\n"
     ]
    }
   ],
   "source": [
    "#тестирование динамического спуска\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Определение функции динамического изменения learning_rate\n",
    "dynamic_lr = lambda iter: 0.5 * (0.85 ** iter)\n",
    "\n",
    "# Создание модели с динамическим шагом обучения\n",
    "model_dynamic_lr = MyLogReg(n_iter=1000, learning_rate=dynamic_lr, metric='accuracy')\n",
    "\n",
    "# Обучение модели\n",
    "model_dynamic_lr.fit(X_train, y_train, verbose=100)\n",
    "\n",
    "# Предсказания и вывод точности\n",
    "y_pred_dynamic_lr = model_dynamic_lr.predict(X_test)\n",
    "accuracy_dynamic_lr = model_dynamic_lr.get_best_score()\n",
    "print(f\"Accuracy with dynamic learning rate: {accuracy_dynamic_lr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loss: 1.02\n",
      "100 loss: 0.58\n",
      "200 loss: 0.52\n",
      "300 loss: 0.47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5624206603443257"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#тестирование для fit\n",
    "X = pd.DataFrame({\n",
    "    'feature1': [0.2, 0.3, 0.5, 0.7],\n",
    "    'feature2': [0.5, 0.6, 0.8, 0.9]\n",
    "})\n",
    "y = pd.Series([0, 0, 1, 1])\n",
    "\n",
    "# Инициализируем и обучим модель\n",
    "model = MyLogReg(n_iter=300, learning_rate=0.1)\n",
    "model.fit(X, y, verbose=100)\n",
    "\n",
    "# Получим веса\n",
    "weights = model.get_coef()\n",
    "np.mean(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1: n_samples=400, n_informative=5\n",
      "Start loss: 1.82\n",
      "Сумма предсказанных классов: 197\n",
      "Среднее вероятностей: 0.502369\n",
      "\n",
      "Dataset 2: n_samples=500, n_informative=7\n",
      "Start loss: 2.54\n",
      "Сумма предсказанных классов: 256\n",
      "Среднее вероятностей: 0.504064\n",
      "\n",
      "Dataset 3: n_samples=300, n_informative=3\n",
      "Start loss: 0.82\n",
      "Сумма предсказанных классов: 154\n",
      "Среднее вероятностей: 0.500393\n"
     ]
    }
   ],
   "source": [
    "# тестировка для predict\n",
    "model = MyLogReg(n_iter=1000, learning_rate=0.1)\n",
    "\n",
    "# Генерация данных\n",
    "def test_model(n_samples, n_informative):\n",
    "    X, y = make_classification(n_samples=n_samples, n_features=10, n_informative=n_informative, \n",
    "                               n_redundant=0, random_state=42)\n",
    "    \n",
    "    # Преобразуем данные в pandas DataFrame для удобства\n",
    "    X_df = pd.DataFrame(X)\n",
    "    y_series = pd.Series(y)\n",
    "    \n",
    "    # Обучаем модель\n",
    "    model.fit(X_df, y_series, verbose=False)\n",
    "    \n",
    "    # Получаем предсказания\n",
    "    predictions = model.predict(X_df)\n",
    "    probabilities = model.predict_proba(X_df)\n",
    "    \n",
    "    # Выводим результаты\n",
    "    print(f\"Сумма предсказанных классов: {np.sum(predictions)}\")\n",
    "    print(f\"Среднее вероятностей: {np.mean(probabilities):.6f}\")\n",
    "\n",
    "# Пример 1: n_samples=400, n_informative=5\n",
    "print(\"Dataset 1: n_samples=400, n_informative=5\")\n",
    "test_model(n_samples=400, n_informative=5)\n",
    "\n",
    "# Пример 2: n_samples=500, n_informative=7\n",
    "print(\"\\nDataset 2: n_samples=500, n_informative=7\")\n",
    "test_model(n_samples=500, n_informative=7)\n",
    "\n",
    "# Пример 3: n_samples=300, n_informative=3\n",
    "print(\"\\nDataset 3: n_samples=300, n_informative=3\")\n",
    "test_model(n_samples=300, n_informative=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loss: 1.40 i f1: 0.62\n",
      "10 loss: 1.12, f1: 0.62\n",
      "20 loss: 0.92, f1: 0.65\n",
      "30 loss: 0.78, f1: 0.67\n",
      "40 loss: 0.68, f1: 0.67\n",
      "50 loss: 0.61, f1: 0.69\n",
      "60 loss: 0.56, f1: 0.72\n",
      "70 loss: 0.52, f1: 0.74\n",
      "80 loss: 0.50, f1: 0.76\n",
      "90 loss: 0.48, f1: 0.78\n",
      "100 loss: 0.47, f1: 0.79\n",
      "Лучшее значение f1-score: 0.7879\n"
     ]
    }
   ],
   "source": [
    "# тестирование метрик\n",
    "X, y = make_classification(n_samples=400, n_features=10, n_informative=5, random_state=42)\n",
    "\n",
    "# Преобразуем данные в pandas DataFrame\n",
    "X_df = pd.DataFrame(X)\n",
    "y_series = pd.Series(y)\n",
    "\n",
    "# Инициализируем модель с метрикой f1\n",
    "model = MyLogReg(n_iter=100, learning_rate=0.1, metric='f1')\n",
    "\n",
    "# Обучаем модель\n",
    "model.fit(X_df, y_series, verbose=10)\n",
    "\n",
    "# Получаем метрику f1 после обучения\n",
    "best_f1_score = model.get_best_score()\n",
    "print(f\"Лучшее значение f1-score: {best_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loss: 2.47 i accuracy: 0.54\n",
      "100 loss: 0.45, accuracy: 0.80\n",
      "200 loss: 0.43, accuracy: 0.83\n",
      "300 loss: 0.43, accuracy: 0.83\n",
      "400 loss: 0.43, accuracy: 0.83\n",
      "500 loss: 0.43, accuracy: 0.83\n",
      "600 loss: 0.43, accuracy: 0.83\n",
      "700 loss: 0.43, accuracy: 0.83\n",
      "800 loss: 0.43, accuracy: 0.83\n",
      "900 loss: 0.43, accuracy: 0.83\n",
      "1000 loss: 0.43, accuracy: 0.83\n",
      "Sum of coefficients without regularization: 5.2989\n",
      "Start loss: 2.57 i accuracy: 0.54\n",
      "100 loss: 0.49, accuracy: 0.80\n",
      "200 loss: 0.47, accuracy: 0.83\n",
      "300 loss: 0.47, accuracy: 0.83\n",
      "400 loss: 0.47, accuracy: 0.83\n",
      "500 loss: 0.47, accuracy: 0.83\n",
      "600 loss: 0.47, accuracy: 0.82\n",
      "700 loss: 0.47, accuracy: 0.83\n",
      "800 loss: 0.47, accuracy: 0.83\n",
      "900 loss: 0.47, accuracy: 0.83\n",
      "1000 loss: 0.47, accuracy: 0.83\n",
      "Sum of coefficients with L1 regularization: 3.7181\n",
      "Start loss: 2.57 i accuracy: 0.54\n",
      "100 loss: 0.48, accuracy: 0.80\n",
      "200 loss: 0.46, accuracy: 0.83\n",
      "300 loss: 0.46, accuracy: 0.83\n",
      "400 loss: 0.46, accuracy: 0.83\n",
      "500 loss: 0.46, accuracy: 0.83\n",
      "600 loss: 0.45, accuracy: 0.83\n",
      "700 loss: 0.45, accuracy: 0.83\n",
      "800 loss: 0.45, accuracy: 0.83\n",
      "900 loss: 0.45, accuracy: 0.83\n",
      "1000 loss: 0.45, accuracy: 0.83\n",
      "Sum of coefficients with L2 regularization: 3.6244\n",
      "Start loss: 2.67 i accuracy: 0.54\n",
      "100 loss: 0.51, accuracy: 0.81\n",
      "200 loss: 0.49, accuracy: 0.83\n",
      "300 loss: 0.48, accuracy: 0.83\n",
      "400 loss: 0.48, accuracy: 0.83\n",
      "500 loss: 0.48, accuracy: 0.83\n",
      "600 loss: 0.48, accuracy: 0.83\n",
      "700 loss: 0.48, accuracy: 0.83\n",
      "800 loss: 0.48, accuracy: 0.83\n",
      "900 loss: 0.48, accuracy: 0.83\n",
      "1000 loss: 0.48, accuracy: 0.83\n",
      "Sum of coefficients with ElasticNet regularization: 3.0812\n"
     ]
    }
   ],
   "source": [
    "# тестирование регуляризации\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборку\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Тест 1: Логистическая регрессия без регуляризации\n",
    "model_no_reg = MyLogReg(n_iter=1000, learning_rate=0.1, metric='accuracy', reg=None)\n",
    "model_no_reg.fit(X_train, y_train, verbose=100)\n",
    "y_pred_no_reg = model_no_reg.predict(X_test)\n",
    "accuracy_no_reg = model_no_reg.get_best_score()\n",
    "coef_sum_no_reg = np.sum(np.abs(model_no_reg.get_coef()))  # Сумма коэффициентов\n",
    "print(f\"Sum of coefficients without regularization: {coef_sum_no_reg:.4f}\")\n",
    "\n",
    "# Тест 2: Логистическая регрессия с L1 регуляризацией (Lasso)\n",
    "model_l1_reg = MyLogReg(n_iter=1000, learning_rate=0.1, metric='accuracy', reg='l1', l1_coef=0.01)\n",
    "model_l1_reg.fit(X_train, y_train, verbose=100)\n",
    "y_pred_l1 = model_l1_reg.predict(X_test)\n",
    "accuracy_l1 = model_l1_reg.get_best_score()\n",
    "coef_sum_l1 = np.sum(np.abs(model_l1_reg.get_coef()))  # Сумма коэффициентов\n",
    "print(f\"Sum of coefficients with L1 regularization: {coef_sum_l1:.4f}\")\n",
    "\n",
    "# Тест 3: Логистическая регрессия с L2 регуляризацией (Ridge)\n",
    "model_l2_reg = MyLogReg(n_iter=1000, learning_rate=0.1, metric='accuracy', reg='l2', l2_coef=0.01)\n",
    "model_l2_reg.fit(X_train, y_train, verbose=100)\n",
    "y_pred_l2 = model_l2_reg.predict(X_test)\n",
    "accuracy_l2 = model_l2_reg.get_best_score()\n",
    "coef_sum_l2 = np.sum(np.abs(model_l2_reg.get_coef()))  # Сумма коэффициентов\n",
    "print(f\"Sum of coefficients with L2 regularization: {coef_sum_l2:.4f}\")\n",
    "\n",
    "# Тест 4: Логистическая регрессия с ElasticNet регуляризацией\n",
    "model_elasticnet_reg = MyLogReg(n_iter=1000, learning_rate=0.1, metric='accuracy', reg='elasticnet', l1_coef=0.01, l2_coef=0.01)\n",
    "model_elasticnet_reg.fit(X_train, y_train, verbose=100)\n",
    "y_pred_elasticnet = model_elasticnet_reg.predict(X_test)\n",
    "accuracy_elasticnet = model_elasticnet_reg.get_best_score()\n",
    "coef_sum_elasticnet = np.sum(np.abs(model_elasticnet_reg.get_coef()))  # Сумма коэффициентов\n",
    "print(f\"Sum of coefficients with ElasticNet regularization: {coef_sum_elasticnet:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
