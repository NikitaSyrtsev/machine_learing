{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forest_class import MyForestClf\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBoostClf:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators=10,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        min_samples_split=2,\n",
    "        max_leafs=20,\n",
    "        bins=16,\n",
    "        metric=None,  # параметр для метрики\n",
    "        max_features=0.5, # стохаст. градиентный бустинг\n",
    "        max_samples=0.5,\n",
    "        random_state=42,\n",
    "        reg=0.1,  # Новый параметр регуляризации\n",
    "        early_stopping=None,  # Параметр ранней остановки\n",
    "    ):\n",
    "        # Сохранение параметров как атрибутов класса\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_leafs = max_leafs\n",
    "        self.bins = bins\n",
    "        \n",
    "        self.pred_0 = None  # Изначальное предсказание (среднее по таргету)\n",
    "        self.trees = []  # Список для хранения обученных деревьев\n",
    "        self.metric = metric  # Метрика для оценки\n",
    "        self.best_score = None  # Лучший результат\n",
    "        self.random_state = random_state\n",
    "        self.max_features = max_features\n",
    "        self.max_samples = max_samples\n",
    "        self.reg = reg\n",
    "        self.leafs_cnt = 0  # Инициализация счётчика листьев\n",
    "        self.fi = {}  # Словарь для хранения важности фичей\n",
    "        self.early_stopping = early_stopping  # Сохраняем параметр ранней остановки\n",
    "\n",
    "    def __str__(self):\n",
    "        # Формируем строку с параметрами экземпляра\n",
    "        params = vars(self)\n",
    "        params_str = ', '.join(f\"{key}={value}\" for key, value in params.items())\n",
    "        return f\"MyBoostClf class: {params_str}\"\n",
    "    \n",
    "    def _get_learning_rate(self, step):\n",
    "        #Возвращает значение learning_rate для текущего шага\n",
    "        if callable(self.learning_rate):  # Если learning_rate - лямбда-функция\n",
    "            return self.learning_rate(step)\n",
    "        return self.learning_rate  # Иначе возвращаем фиксированное значение\n",
    "    \n",
    "    def _get_metric_score(self, y_true, y_pred):\n",
    "        if self.metric == 'accuracy':\n",
    "            return accuracy_score(y_true, y_pred)\n",
    "        elif self.metric == 'precision':\n",
    "            return precision_score(y_true, y_pred)\n",
    "        elif self.metric == 'recall':\n",
    "            return recall_score(y_true, y_pred)\n",
    "        elif self.metric == 'f1':\n",
    "            return f1_score(y_true, y_pred)\n",
    "        elif self.metric == 'roc_auc':\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "        else:\n",
    "            return None  # Если метрика не указана\n",
    "        \n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series, X_eval=None, y_eval=None, early_stopping=None, verbose=None):\n",
    "        # Фиксируем seed\n",
    "        random.seed(self.random_state)\n",
    "\n",
    "        # Преобразуем X в DataFrame, если это ndarray\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        # Преобразуем y в Series, если это ndarray\n",
    "        if isinstance(y, np.ndarray):\n",
    "            y = pd.Series(y)\n",
    "\n",
    "        self.pred_0 = y.mean()  # Изначальное предсказание (среднее по таргету)\n",
    "        current_prediction = np.full(y.shape, self.pred_0)  # Инициализируем предсказания\n",
    "\n",
    "        init_cols = list(X.columns)  # Список всех колонок\n",
    "        init_rows_cnt = X.shape[0]  # Общее количество строк\n",
    "\n",
    "        # Инициализируем важность фичей\n",
    "        self.fi = {col: 0.0 for col in init_cols}\n",
    "\n",
    "        # Для ранней остановки\n",
    "        best_eval_score = None\n",
    "        epochs_without_improvement = 0\n",
    "        best_trees = []\n",
    "\n",
    "        # Основной цикл для обучения деревьев\n",
    "        for step in range(1, self.n_estimators + 1):\n",
    "            # Определяем количество колонок и строк для подвыборки\n",
    "            cols_smpl_cnt = round(len(init_cols) * self.max_features)\n",
    "            rows_smpl_cnt = round(init_rows_cnt * self.max_samples)\n",
    "\n",
    "            # Генерируем случайные индексы колонок и строк\n",
    "            cols_idx = random.sample(init_cols, cols_smpl_cnt)\n",
    "            rows_idx = random.sample(range(init_rows_cnt), rows_smpl_cnt)\n",
    "\n",
    "            # Создаем подвыборку данных\n",
    "            X_sub = X.iloc[rows_idx][cols_idx]\n",
    "            y_sub = y.iloc[rows_idx]\n",
    "\n",
    "            # Вычисляем остатки (градиенты логистической функции потерь)\n",
    "            residuals = y_sub - 1 / (1 + np.exp(-current_prediction[rows_idx]))\n",
    "\n",
    "            # Обучаем дерево на остатках\n",
    "            tree = MyForestClf(\n",
    "                n_estimators=1,\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                max_leafs=self.max_leafs,\n",
    "                bins=self.bins\n",
    "            )\n",
    "            tree.fit(X_sub, residuals)\n",
    "            # Добавляем дерево в список\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            # Обновляем важность фичей (из каждого дерева)\n",
    "            tree_feature_importances = tree.feature_importances()\n",
    "            for feature, importance in tree_feature_importances.items():\n",
    "                self.fi[feature] += importance  # Суммируем важность фичей\n",
    "\n",
    "            # Подсчет листьев и регуляризационный штраф\n",
    "            num_leaves = tree.leafs_cnt  # Используем листы из текущего дерева\n",
    "            self.leafs_cnt += num_leaves  # Обновляем общее количество листьев\n",
    "            reg_penalty = self.reg * num_leaves  # Регуляризационный штраф для текущего дерева\n",
    "\n",
    "            # Обновляем предсказания с учетом регуляризации\n",
    "            tree_predictions = np.array(tree.predict(X))\n",
    "            current_learning_rate = self._get_learning_rate(step)\n",
    "            current_prediction += current_learning_rate * tree_predictions - reg_penalty\n",
    "\n",
    "            # Логирование метрик\n",
    "            if verbose and step % verbose == 0:\n",
    "                y_pred = (1 / (1 + np.exp(-current_prediction)) > 0.5).astype(int)\n",
    "                score = self._get_metric_score(y, y_pred) if self.metric else None\n",
    "                print(f\"{step}. LogLoss: {self._log_loss(y, 1 / (1 + np.exp(-current_prediction))):.6f} | \"\n",
    "                      f\"Регуляризационный штраф: {reg_penalty:.4f} | \"\n",
    "                      f\"{self.metric.capitalize() if self.metric else 'N/A'}: {score:.2f}\")\n",
    "\n",
    "            # Оценка на валидационных данных для ранней остановки\n",
    "            if X_eval is not None and y_eval is not None and early_stopping is not None:\n",
    "                y_pred_eval = (1 / (1 + np.exp(-current_prediction)) > 0.5).astype(int)\n",
    "                eval_score = self._get_metric_score(y_eval, y_pred_eval) if self.metric else self._log_loss(y_eval, 1 / (1 + np.exp(-current_prediction)))\n",
    "                print(f\"Validation score at step {step}: {eval_score:.4f}\")\n",
    "\n",
    "                if best_eval_score is None or eval_score > best_eval_score:  # Для метрики с максимизацией\n",
    "                    best_eval_score = eval_score\n",
    "                    epochs_without_improvement = 0\n",
    "                    best_trees = self.trees.copy()\n",
    "                else:\n",
    "                    epochs_without_improvement += 1\n",
    "\n",
    "                # Если улучшения нет в течение `early_stopping` шагов, останавливаем обучение\n",
    "                if epochs_without_improvement >= early_stopping:\n",
    "                    print(f\"Early stopping triggered at step {step}.\")\n",
    "                    self.trees = best_trees  # Откатываем модель к лучшему состоянию\n",
    "                    break\n",
    "\n",
    "        # Нормализуем важность фичей\n",
    "        total_importance = sum(self.fi.values())\n",
    "        if total_importance > 0:\n",
    "            for feature in self.fi:\n",
    "                self.fi[feature] /= total_importance\n",
    "\n",
    "        # Финальная метрика по окончанию обучения\n",
    "        y_pred_final = (1 / (1 + np.exp(-current_prediction)) > 0.5).astype(int)\n",
    "        if self.metric:\n",
    "            self.best_score = self._get_metric_score(y, y_pred_final)\n",
    "        else:\n",
    "            self.best_score = self._log_loss(y, 1 / (1 + np.exp(-current_prediction)))\n",
    "\n",
    "\n",
    "    def _log_loss(self, y_true, y_pred, eps=1e-15):\n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps)  # Ограничиваем предсказания\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # Преобразуем X в DataFrame, если это ndarray\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "        \n",
    "        # Начальные предсказания\n",
    "        F_x = np.full((X.shape[0],), self.pred_0)\n",
    "        \n",
    "        # Предсказания деревьев\n",
    "        for step, tree in enumerate(self.trees, start=1):\n",
    "            tree_predictions = np.array(tree.predict(X))\n",
    "            \n",
    "            # Рассчитываем коэффициент learning_rate для текущего шага\n",
    "            if callable(self.learning_rate):\n",
    "                lr = self.learning_rate(step)  # Вычисление динамического learning_rate\n",
    "            else:\n",
    "                lr = self.learning_rate  # Статическое значение\n",
    "            \n",
    "            F_x += lr * tree_predictions  # Обновляем предсказания\n",
    "        \n",
    "        # Преобразуем логарифмические шансы в вероятности\n",
    "        odds = np.exp(F_x)\n",
    "        probs = odds / (1 + odds)\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        # Преобразуем вероятности в классы, используя порог 0.5\n",
    "        return (probs > 0.5).astype(int)\n",
    "    \n",
    "    def feature_importances(self):\n",
    "        return self.fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score at step 1: 0.5000\n",
      "Validation score at step 2: 0.5000\n",
      "Validation score at step 3: 0.5000\n",
      "Validation score at step 4: 0.5000\n",
      "Early stopping triggered at step 4.\n",
      "Лучший результат (best_score): 0.5000\n"
     ]
    }
   ],
   "source": [
    "#Тест остановки\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Параметры модели\n",
    "early_stopping = 3  # Останавливаем обучение, если не будет улучшений в течение 3 шагов\n",
    "metric = 'accuracy'  # Используем метрику точности (accuracy)\n",
    "\n",
    "# Инициализируем модель\n",
    "model = MyBoostClf(n_estimators=100, learning_rate=0.1, metric=metric, max_depth=5, early_stopping=early_stopping)\n",
    "\n",
    "# Обучаем модель с параметрами ранней остановки и на всех данных\n",
    "model.fit(X, y, X_eval=X, y_eval=y, early_stopping=early_stopping, verbose=10)\n",
    "\n",
    "# Выводим лучший результат\n",
    "print(f\"Лучший результат (best_score): {model.best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Важность фичей: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0}\n"
     ]
    }
   ],
   "source": [
    "#Тест важности фичей\n",
    "X, y = make_classification(n_samples=300, n_features=10, random_state=41)\n",
    "\n",
    "# Инициализируем модель\n",
    "model = MyBoostClf(n_estimators=5, max_depth=3, learning_rate=0.4)\n",
    "\n",
    "# Обучаем модель\n",
    "model.fit(X, y)\n",
    "\n",
    "# Получаем важность фичей\n",
    "feature_importances = model.feature_importances()\n",
    "print(\"Важность фичей:\", feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No regularization best_score: 0.5\n",
      "Moderate regularization best_score: 0.5\n",
      "High regularization best_score: 0.5\n"
     ]
    }
   ],
   "source": [
    "#Тест регуляризации\n",
    "X, y = make_classification(n_samples=40, n_features=10, random_state=42)\n",
    "y = pd.Series(y)\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "# Тест без регуляризации\n",
    "model_no_reg = MyBoostClf(n_estimators=10, learning_rate=0.1, metric='accuracy', reg=0.0, random_state=42)\n",
    "model_no_reg.fit(X, y)\n",
    "print(f\"No regularization best_score: {model_no_reg.best_score}\")\n",
    "\n",
    "# Тест с умеренной регуляризацией\n",
    "model_moderate_reg = MyBoostClf(n_estimators=10, learning_rate=0.1, metric='accuracy', reg=0.1, random_state=42)\n",
    "model_moderate_reg.fit(X, y)\n",
    "print(f\"Moderate regularization best_score: {model_moderate_reg.best_score}\")\n",
    "\n",
    "# Тест с сильной регуляризацией\n",
    "model_high_reg = MyBoostClf(n_estimators=10, learning_rate=0.1, metric='accuracy', reg=1.0, random_state=42)\n",
    "model_high_reg.fit(X, y)\n",
    "print(f\"High regularization best_score: {model_high_reg.best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed learning_rate predictions sum: 100\n",
      "Linear decreasing learning_rate predictions sum: 100\n",
      "Exponential decreasing learning_rate predictions sum: 100\n"
     ]
    }
   ],
   "source": [
    "#Тест learning rate\n",
    "X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
    "y = pd.Series(y)\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "# Тест с фиксированным learning_rate\n",
    "model_fixed = MyBoostClf(n_estimators=10, learning_rate=0.1, metric='accuracy', random_state=42)\n",
    "model_fixed.fit(X, y)\n",
    "predictions_fixed = model_fixed.predict(X)\n",
    "print(f\"Fixed learning_rate predictions sum: {np.sum(predictions_fixed)}\")\n",
    "\n",
    "# Тест с линейно уменьшающимся learning_rate\n",
    "model_linear = MyBoostClf(n_estimators=10, learning_rate=lambda step: 0.5 / step, metric='accuracy', random_state=42)\n",
    "model_linear.fit(X, y)\n",
    "predictions_linear = model_linear.predict(X)\n",
    "print(f\"Linear decreasing learning_rate predictions sum: {np.sum(predictions_linear)}\")\n",
    "\n",
    "# Тест с экспоненциально уменьшающимся learning_rate\n",
    "model_exponential = MyBoostClf(n_estimators=10, learning_rate=lambda step: 0.1 * (0.9 ** step), metric='accuracy', random_state=42)\n",
    "model_exponential.fit(X, y)\n",
    "predictions_exponential = model_exponential.predict(X)\n",
    "print(f\"Exponential decreasing learning_rate predictions sum: {np.sum(predictions_exponential)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1\n",
      "Best accuracy score: 0.5000\n",
      "\n",
      "Training model 2\n",
      "Best precision score: 0.5000\n",
      "\n",
      "Training model 3\n",
      "Best recall score: 1.0000\n",
      "\n",
      "Training model 4\n",
      "Best f1 score: 0.6667\n",
      "\n",
      "Training model 5\n",
      "Best roc_auc score: 0.5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Тест стохастического градиентного бустинга\n",
    "X, y = make_classification(n_samples=500, n_features=10, n_classes=2, random_state=42)\n",
    "\n",
    "# Список наборов параметров для тестирования\n",
    "param_sets = [\n",
    "    {\"n_estimators\": 10, \"learning_rate\": 0.1, \"max_depth\": 3, \"max_features\": 0.6, \"max_samples\": 0.7, \"metric\": \"accuracy\"},\n",
    "    {\"n_estimators\": 15, \"learning_rate\": 0.05, \"max_depth\": 5, \"max_features\": 0.5, \"max_samples\": 0.6, \"metric\": \"precision\"},\n",
    "    {\"n_estimators\": 20, \"learning_rate\": 0.1, \"max_depth\": 4, \"max_features\": 0.4, \"max_samples\": 0.5, \"metric\": \"recall\"},\n",
    "    {\"n_estimators\": 25, \"learning_rate\": 0.1, \"max_depth\": 3, \"max_features\": 0.7, \"max_samples\": 0.8, \"metric\": \"f1\"},\n",
    "    {\"n_estimators\": 30, \"learning_rate\": 0.15, \"max_depth\": 6, \"max_features\": 0.6, \"max_samples\": 0.5, \"metric\": \"roc_auc\"}\n",
    "]\n",
    "\n",
    "# Обучение моделей с разными параметрами и вывод best_score\n",
    "for i, params in enumerate(param_sets):\n",
    "    print(f\"Training model {i+1}\")\n",
    "    \n",
    "    # Создание и обучение модели\n",
    "    model = MyBoostClf(**params)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Вывод лучшей метрики\n",
    "    print(f\"Best {params['metric']} score: {model.best_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing with metric: accuracy ===\n",
      "Final accuracy: 0.5000\n",
      "=== Testing with metric: precision ===\n",
      "Final precision: 0.5000\n",
      "=== Testing with metric: recall ===\n",
      "Final recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#Тестирование метрик\n",
    "X, y = make_classification(\n",
    "    n_samples=100,  # Общее количество выборок\n",
    "    n_features=10,  # Количество признаков\n",
    "    n_classes=2,    # Бинарная классификация\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Список метрик для тестирования\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\"]\n",
    "# Параметры для модели\n",
    "params = {\n",
    "    \"n_estimators\": 10,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 3,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"max_leafs\": 10,\n",
    "    \"bins\": 16,\n",
    "}\n",
    "# Тестирование модели с разными метриками\n",
    "for metric in metrics:\n",
    "    print(f\"=== Testing with metric: {metric} ===\")\n",
    "    params[\"metric\"] = metric  # Задаем текущую метрику\n",
    "    model = MyBoostClf(**params)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    probs = model.predict_proba(X)\n",
    "    preds = model.predict(X)\n",
    "    # Итоговая метрика\n",
    "    print(f\"Final {metric}: {model.best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities: [0.62245933 0.62245933 0.62245933 0.62245933 0.62245933]\n",
      "Predicted classes: [1 1 1 1 1]\n",
      "Total leaf count: 20\n"
     ]
    }
   ],
   "source": [
    "#Тестирование предсказания\n",
    "X, y = make_classification(n_samples=50, n_features=10, n_classes=2, random_state=42)\n",
    "\n",
    "# Параметры для модели\n",
    "params = {\n",
    "    \"n_estimators\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 3,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"max_leafs\": 10,\n",
    "    \"bins\": 16\n",
    "}\n",
    "\n",
    "# Обучение модели\n",
    "model = MyBoostClf(**params)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Получение вероятностей\n",
    "probs = model.predict_proba(X)\n",
    "print(f\"Predicted probabilities: {probs[:5]}\")  # Печатаем первые 5 вероятностей\n",
    "\n",
    "# Получение предсказанных классов\n",
    "predictions = model.predict(X)\n",
    "print(f\"Predicted classes: {predictions[:5]}\")  # Печатаем первые 5 предсказанных классов\n",
    "\n",
    "# Сумма листьев всех деревьев\n",
    "leaf_count = sum(tree.leafs_cnt for tree in model.trees)\n",
    "print(f\"Total leaf count: {leaf_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 - pred_0: 0.50\n",
      "Model 1 - Total leaf count: 20\n",
      "Model 2 - pred_0: 0.48\n",
      "Model 2 - Total leaf count: 20\n"
     ]
    }
   ],
   "source": [
    "#Тестирование обучения\n",
    "X1, y1 = make_classification(n_samples=50, n_features=10, n_classes=2, random_state=42)\n",
    "\n",
    "# Параметры для модели\n",
    "params = {\n",
    "    \"n_estimators\": 5,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 3,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"max_leafs\": 10,\n",
    "    \"bins\": 16\n",
    "}\n",
    "\n",
    "# Обучение модели на первом наборе данных\n",
    "model1 = MyBoostClf(**params)\n",
    "model1.fit(X1, y1)\n",
    "print(f\"Model 1 - pred_0: {model1.pred_0:.2f}\")\n",
    "leaf_count1 = sum(tree.leafs_cnt for tree in model1.trees)\n",
    "print(f\"Model 1 - Total leaf count: {leaf_count1}\")\n",
    "\n",
    "\n",
    "X2, y2 = make_classification(n_samples=50, n_features=5, n_classes=2, random_state=43)\n",
    "# Обучение модели на втором наборе данных\n",
    "model2 = MyBoostClf(**params)\n",
    "model2.fit(X2, y2)\n",
    "print(f\"Model 2 - pred_0: {model2.pred_0:.2f}\")\n",
    "leaf_count2 = sum(tree.leafs_cnt for tree in model2.trees)\n",
    "print(f\"Model 2 - Total leaf count: {leaf_count2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. LogLoss: 0.724077\n",
      "2. LogLoss: 0.724077\n",
      "3. LogLoss: 0.724077\n",
      "4. LogLoss: 0.724077\n",
      "5. LogLoss: 0.724077\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Тестирование класса\n",
    "X, y = make_classification(n_samples=20, n_features=5, random_state=42)\n",
    "X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
    "y = pd.Series(y)\n",
    "\n",
    "# Создание модели\n",
    "model = MyBoostClf(n_estimators=5, learning_rate=0.1, max_depth=3)\n",
    "\n",
    "# Обучение\n",
    "model.fit(X, y, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyBoostClf class: n_estimators=10, learning_rate=0.1, max_depth=5, min_samples_split=2, max_leafs=20, bins=16, pred_0=None, trees=[], metric=None\n",
      "MyBoostClf class: n_estimators=10, learning_rate=0.1, max_depth=5, min_samples_split=5, max_leafs=10, bins=6, pred_0=None, trees=[], metric=None\n",
      "MyBoostClf class: n_estimators=8, learning_rate=0.1, max_depth=5, min_samples_split=5, max_leafs=20, bins=16, pred_0=None, trees=[], metric=None\n"
     ]
    }
   ],
   "source": [
    "# Тестирование класса\n",
    "model1 = MyBoostClf()\n",
    "model2 = MyBoostClf(n_estimators=10,  max_depth=5, min_samples_split=5, max_leafs=10, bins=6)\n",
    "model3 = MyBoostClf(n_estimators=8, max_depth=5, min_samples_split=5, max_leafs=20, bins=16)\n",
    "\n",
    "# Проверка\n",
    "print(model1)\n",
    "print(model2)\n",
    "print(model3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
